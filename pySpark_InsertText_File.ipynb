{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpy55sglbM5o"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iHyli3UbTHx"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"LoadDataIntoHive\").enableHiveSupport().getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRenaZX2bWo4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Define the schema for the Hive table\n",
        "schema = StructType([\n",
        "    StructField(\"data\", StringType(), True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGdbWvtQbasA"
      },
      "outputs": [],
      "source": [
        "# Create a Hive table\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS my_hive_table (data STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW2PYlbAbczx"
      },
      "outputs": [],
      "source": [
        "# Load data from the text file into a DataFrame\n",
        "data_file_path = \"path_to_your_text_file.txt\"\n",
        "data_df = spark.read.csv(data_file_path, schema=schema, header=False, sep=\",\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwNIdt-mbdeb"
      },
      "outputs": [],
      "source": [
        "# Insert the data into the Hive table\n",
        "data_df.write.insertInto(\"my_hive_table\")\n",
        "data_df.write.saveAsTable('my_hive_table')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtctSSJybfmT"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
